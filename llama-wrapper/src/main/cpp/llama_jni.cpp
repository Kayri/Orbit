#include <jni.h>
#include <string>
#include <vector>
#include <android/log.h>
#include "llama.h"

#define LOG_TAG "LlamaJNI"
#define LOGI(...) __android_log_print(ANDROID_LOG_INFO, LOG_TAG, __VA_ARGS__)

static struct llama_context *ctx = nullptr;
static llama_model *model = nullptr;

/**
 * Initializes the llama.cpp model and context for future inference.
 *
 * @param modelPath A string representing the absolute path to the .gguf model file.
 *
 * This function loads the model into memory and prepares the context
 * so that runPrompt() can be called later.
 */
extern "C"
JNIEXPORT void JNICALL
Java_com_mehdiatique_llamawrapper_LlamaBridge_initLlama(JNIEnv *env, jobject, jstring modelPath) {
    const char *path = env->GetStringUTFChars(modelPath, nullptr);

    llama_backend_init();

    llama_model_params model_params = llama_model_default_params();
    model = llama_model_load_from_file(path, model_params);

    llama_context_params ctx_params = llama_context_default_params();
    ctx = llama_init_from_model(model, ctx_params);

    LOGI("Model loaded from path: %s", path);

    env->ReleaseStringUTFChars(modelPath, path);
}


/**
 * Releases the llama context and model to free memory.
 *
 * Should be called when the app is shutting down or when you no longer
 * need the model loaded.
 */
extern "C"
JNIEXPORT void JNICALL
Java_com_mehdiatique_llamawrapper_LlamaBridge_releaseLlama(JNIEnv *, jobject) {
    if (ctx) {
        llama_free(ctx);
        ctx = nullptr;
    }
    if (model) {
        llama_model_free(model);
        model = nullptr;
    }
    llama_backend_free();
}


/**
 * Resets the llama context to clear previous conversation state.
 *
 * The model remains loaded, but memory and token history are reset.
 * Use this to start a fresh chat session without reloading the model.
 */
extern "C"
JNIEXPORT void JNICALL
Java_com_mehdiatique_llamawrapper_LlamaBridge_resetContext(JNIEnv *, jobject){}


/**
 * Generates a response from the model given a prompt string.
 *
 * @param prompt A UTF-8 encoded string to send to the model.
 * @return A UTF-8 encoded response string generated by the model.
 *
 * This function tokenizes the prompt, evaluates it using llama.cpp,
 * and samples output tokens to form a final response.
 */
extern "C"
JNIEXPORT jstring JNICALL
Java_com_mehdiatique_llamawrapper_LlamaBridge_runPrompt(JNIEnv *env, jobject, jstring prompt) {
    const char *input = env->GetStringUTFChars(prompt, nullptr);

    if (!ctx || !model) {
        LOGI("Llama not initialized - No context or model");
        return env->NewStringUTF("Error: Llama not initialized - No context or model");
    }

    // ----- Settings -----
    const int max_generated_tokens = 50;     // Maximum number of tokens the model will generate in the response
    const float temperature = 0.8f;          // Sampling temperature: lower = more deterministic, higher = more creative/random
    const float top_p = 0.95f;               // Nucleus sampling: keep tokens with cumulative probability <= top_p
    const int top_k = 40;                    // Top-K sampling: randomly choose from the top_k most likely tokens
    const llama_vocab *vocab = llama_model_get_vocab(model);
    const llama_token eos_token = llama_vocab_eos(vocab);

    // ----- 1. Tokenize the prompt -----
    std::vector<llama_token> input_tokens;
    {
        size_t raw_len = strlen(input);
        if (raw_len > INT32_MAX) {
            LOGI("Prompt too long for int32");
            env->ReleaseStringUTFChars(prompt, input);
            return env->NewStringUTF("Input too long");
        }

        auto input_len = static_cast<int32_t>(raw_len);
        auto max_tokens = input_len + 8; // generous estimate
        input_tokens.resize(max_tokens);

        const int n_tokens = llama_tokenize(
                vocab,
                input,
                input_len,
                input_tokens.data(),
                max_tokens,
                true,   // add_special(BOS,EOS)
                false   // parse_special(for now, no)
        );

        if (n_tokens < 0) {
            LOGI("Tokenization failed");
            env->ReleaseStringUTFChars(prompt, input);
            return env->NewStringUTF("Tokenization failed");
        }

        input_tokens.resize(n_tokens);
    }


    // ----- 2. Decode the prompt into model context -----
    llama_batch prompt_batch = {
            .n_tokens = static_cast<int32_t>(input_tokens.size()),
            .token = input_tokens.data(),
            .embd     = nullptr,  // we're using token IDs, not embeddings
            .pos      = nullptr,  // let llama track positions
            .n_seq_id = nullptr,  // assume single sequence
            .seq_id   = nullptr,  // default to seq 0
            .logits   = nullptr  // get logits for last token only
    };

    if (llama_decode(ctx, prompt_batch) != 0) {
        LOGI("Decoding failed");
        env->ReleaseStringUTFChars(prompt, input);
        return env->NewStringUTF("Decoding failed");
    }


    // ----- 3. Initialize sampler chain -----
    llama_sampler_chain_params chain_params = {
            .no_perf = true
    };
    llama_sampler *sampler = llama_sampler_chain_init(chain_params);
    llama_sampler_chain_add(sampler, llama_sampler_init_temp(temperature));
    llama_sampler_chain_add(sampler, llama_sampler_init_top_k(top_k));
    llama_sampler_chain_add(sampler, llama_sampler_init_top_p(top_p, 1));
    llama_sampler_chain_add(sampler, llama_sampler_init_dist(LLAMA_DEFAULT_SEED));


    // ----- 4. Autoregressive token generation -----
    std::string result;
    result.reserve(1024);   //Preallocate space

    for (int i = 0; i < max_generated_tokens; i++) {
        llama_token next_token = llama_sampler_sample(sampler, ctx, -1);
        llama_sampler_accept(sampler, next_token);

        if (next_token == eos_token) {
            LOGI("End of sentence reached");
            break;
        }

        char token_buf[128];
        int len = llama_token_to_piece(vocab, next_token, token_buf, sizeof(token_buf), 0, true);
        if (len > 0) result.append(token_buf, len);

        // Feed generated token back into context
        llama_batch token_batch = {
                .n_tokens = 1,
                .token = &next_token,
                .embd = nullptr,
                .pos =nullptr,
                .n_seq_id = nullptr,
                .seq_id = nullptr,
                .logits = nullptr
        };

        if (llama_decode(ctx, token_batch) != 0) {
            LOGI("Failed to decode next token");
            break;
        }
    }


    LOGI("Prompt processed successfully");
    llama_sampler_free(sampler);
    env->ReleaseStringUTFChars(prompt, input);
    return env->NewStringUTF(result.c_str());
}


/**
 * Sets runtime parameters used during token sampling.
 *
 * @param temperature Controls randomness (0.0 = deterministic, >1.0 = more random)
 * @param top_k Only consider top K most likely tokens
 * @param top_p Only consider tokens until cumulative probability reaches p
 *
 * These parameters affect the quality, style, and creativity of model output.
 */
extern "C"
JNIEXPORT void JNICALL
Java_com_mehdiatique_llamawrapper_LlamaBridge_setParams(JNIEnv *, jobject){}